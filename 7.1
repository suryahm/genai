!pip install torch torchvision matplotlib tqdm 
import torch 
import torch.nn as nn 
import torch.optim as optim 
from torchvision import datasets, transforms 
from torch.utils.data import DataLoader 
import matplotlib.pyplot as plt 
from tqdm import tqdm  # For progress bar 
 
# Hyperparameters 
BATCH_SIZE = 256 
NOISE_DIM = 100 
EPOCHS = 10  # Reduced for quicker runs 
LEARNING_RATE = 0.0002 
 
# Check if GPU is available and use it if possible 
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') 
 
# Data preprocessing and loading 
transform = transforms.Compose([ 
    transforms.ToTensor(), 
    transforms.Normalize((0.5,), (0.5,)) 
]) 
 
train_data = datasets.MNIST(root='mnist_data', train=True, transform=transform, 
download=True) 
train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True) 
 
# Generator model 
class Generator(nn.Module): 
    def __init__(self): 
        super(Generator, self).__init__() 
        self.model = nn.Sequential( 
            nn.Linear(NOISE_DIM, 256), 
            nn.LeakyReLU(0.2), 
            nn.Linear(256, 512), 
            nn.LeakyReLU(0.2), 
            nn.Linear(512, 1024), 
            nn.LeakyReLU(0.2), 
            nn.Linear(1024, 28*28), 
            nn.Tanh() 
34 
 
        ) 
 
    def forward(self, x): 
        return self.model(x).view(-1, 1, 28, 28) 
 
# Discriminator model 
class Discriminator(nn.Module): 
    def __init__(self): 
        super(Discriminator, self).__init__() 
        self.model = nn.Sequential( 
            nn.Linear(28*28, 1024), 
            nn.LeakyReLU(0.2), 
            nn.Linear(1024, 512), 
            nn.LeakyReLU(0.2), 
            nn.Linear(512, 256), 
            nn.LeakyReLU(0.2), 
            nn.Linear(256, 1), 
            nn.Sigmoid() 
        ) 
 
    def forward(self, x): 
        return self.model(x.view(-1, 28*28)) 
 
# Initialize models and move them to the appropriate device 
generator = Generator().to(device) 
discriminator = Discriminator().to(device) 
 
# Loss and optimizers 
criterion = nn.BCELoss() 
optimizer_G = optim.Adam(generator.parameters(), lr=LEARNING_RATE) 
optimizer_D = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE) 
 
# Function to generate and save images 
def generate_and_save_images(epoch, test_input): 
    with torch.no_grad(): 
        fake_images = generator(test_input).detach().cpu() 
    fig = plt.figure(figsize=(4, 4)) 
    for i in range(fake_images.size(0)): 
        plt.subplot(4, 4, i + 1) 
        plt.imshow(fake_images[i, 0, :, :] * 0.5 + 0.5, cmap='gray') 
        plt.axis('off') 
    plt.savefig(f'gan_generated_image_epoch_{epoch}.png') 
    plt.show() 
 
35 
 
# Training loop with progress bar 
for epoch in range(EPOCHS): 
    for real_images, _ in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{EPOCHS}'): 
        real_images = real_images.view(-1, 28*28).to(device) 
        batch_size = real_images.size(0) 
        labels_real = torch.ones(batch_size, 1).to(device) 
        labels_fake = torch.zeros(batch_size, 1).to(device) 
 
        # Train Discriminator 
        outputs_real = discriminator(real_images) 
        loss_real = criterion(outputs_real, labels_real) 
 
        noise = torch.randn(batch_size, NOISE_DIM).to(device) 
        fake_images = generator(noise) 
        outputs_fake = discriminator(fake_images.detach()) 
        loss_fake = criterion(outputs_fake, labels_fake) 
 
        loss_D = loss_real + loss_fake 
        optimizer_D.zero_grad() 
        loss_D.backward() 
        optimizer_D.step() 
 
        # Train Generator 
        outputs = discriminator(fake_images) 
        loss_G = criterion(outputs, labels_real) 
 
        optimizer_G.zero_grad() 
        loss_G.backward() 
        optimizer_G.step() 
 
    print(f'Epoch [{epoch+1}/{EPOCHS}] Loss D: {loss_D.item():.4f}, Loss G: 
{loss_G.item():.4f}') 
 
    # Generate and save images at each epoch 
    if (epoch + 1) % 10 == 0 or epoch == 0: 
        generate_and_save_images(epoch + 1, torch.randn(16, NOISE_DIM).to(device)) 
